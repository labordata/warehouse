# This is a basic workflow to help you get started with Actions

name: Update Database

concurrency: 
  group: database-build


# Controls when the action will run. 
on:
  schedule:
    # * is a special character in YAML so you have to quote this string
    - cron:  '15 10 * * *'    

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      - name: get databases
        run: |
          wget https://github.com/labordata/nlrb-data/releases/download/nightly/nlrb.db.zip
          wget http://labordata.github.io/fmcs-f7/f7.db.zip
          wget https://github.com/labordata/opdr/releases/download/2021-05-31/opdr.db.zip
          wget https://labordata.github.io/nlrb-cats/nlrb.sqlite.zip
          wget https://github.com/labordata/nlrb-voluntary-recognitions/raw/main/voluntary_recognitions.db
          wget https://github.com/labordata/fmcs-work-stoppage/raw/main/work_stoppages.db
          wget https://github.com/labordata/lm20/releases/download/nightly/lm20.db.zip
          wget https://github.com/labordata/CHIPS/releases/download/first-chips/chips.db.zip
          unzip nlrb.db.zip
          unzip f7.db.zip
          unzip opdr.db.zip
          unzip nlrb.sqlite.zip
          mv nlrb.sqlite old_nlrb.db
          unzip lm20.db.zip
          unzip chips.db.zip
          echo "analyze;" | sqlite3 f7.db
          echo "analyze;" | sqlite3 nlrb.db
          echo "analyze;" | sqlite3 opdr.db
          echo "analyze;" | sqlite3 old_nlrb.db
          echo "analyze;" | sqlite3 voluntary_recognitions.db
          echo "analyze;" | sqlite3 work_stoppages.db
          echo "analyze;" | sqlite3 lm20.db
          echo "analyze;" | sqlite3 chips.db


      - name: Set up Cloud Run
        uses: google-github-actions/setup-gcloud@master
        with:
          service_account_email: ${{ secrets.GCP_SA_EMAIL }}
          service_account_key: ${{ secrets.GCP_SA_KEY }}
      - name: Deploy to Cloudrun
        run: |
          pip install datasette==0.61a0
          gcloud config set run/region us-central1
          gcloud config set project labordata          
          datasette publish cloudrun f7.db nlrb.db opdr.db old_nlrb.db voluntary_recognitions.db work_stoppages.db lm20.db chips.db \
            --memory 8Gi \
            --cpu 2 \
            --service warehouse \
            -m warehouse_metadata.yml \
            --extra-options="--crossdb --setting sql_time_limit_ms 50000 --cors --setting facet_time_limit_ms 500 --setting allow_facet off --setting trace_debug 1 --setting max_returned_rows 5000" \
            --install=https://github.com/simonw/datasette-hashed-urls/archive/be554804c0a68e21c3cd52e9aebb287d10d2179e.zip \
            --install=datasette-atom \
            --install=datasette-rure \
            --install=pysqlite3-binary \
            --install=datasette-block-robots \
            --install=datasette-pretty-traces \
            --version-note=$GITHUB_RUN_NUMBER
