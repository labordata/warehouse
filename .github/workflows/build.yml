# This is a basic workflow to help you get started with Actions

name: Update Database

concurrency: 
  group: database-build


# Controls when the action will run. 
on:
  schedule:
    # * is a special character in YAML so you have to quote this string
    - cron:  '15 10 * * *'    

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: dependencies
        run: |
          pip install datasette sqlite-utils
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      - name: get databases
        run: |
          wget https://github.com/labordata/nlrb-data/releases/download/nightly/nlrb.db.zip
          wget http://labordata.github.io/fmcs-f7/f7.db.zip
          wget https://github.com/labordata/opdr/releases/download/2021-05-31/opdr.db.zip
          wget https://labordata.github.io/nlrb-cats/nlrb.sqlite.zip
          wget https://github.com/labordata/nlrb-voluntary-recognitions/raw/main/voluntary_recognitions.db
          wget https://github.com/labordata/fmcs-work-stoppage/raw/main/work_stoppages.db
          wget https://github.com/labordata/lm20/releases/download/nightly/lm20.db.zip
          wget https://github.com/labordata/CHIPS/releases/download/first-chips/chips.db.zip
          unzip nlrb.db.zip
          unzip f7.db.zip
          unzip opdr.db.zip
          unzip nlrb.sqlite.zip
          mv nlrb.sqlite old_nlrb.db
          unzip lm20.db.zip
          unzip chips.db.zip
          echo "analyze;" | sqlite3 f7.db
          echo "analyze;" | sqlite3 nlrb.db
          echo "analyze;" | sqlite3 opdr.db
          echo "analyze;" | sqlite3 old_nlrb.db
          echo "analyze;" | sqlite3 voluntary_recognitions.db
          echo "analyze;" | sqlite3 work_stoppages.db
          echo "analyze;" | sqlite3 lm20.db
          echo "analyze;" | sqlite3 chips.db
          sqlite-utils enable-fts opdr.db lm_data union_name aff_abbr unit_name desiq_pre desig_num desig_suf street_adr city state zip
          sqlite-utils enable-fts f7.db f7 union_name union_street union_city union_state union_zip


      - id: 'auth'
        uses: 'google-github-actions/auth@v0'
        with:
            workload_identity_provider: 'projects/450850437809/locations/global/workloadIdentityPools/gh-pool/providers/gh-provider'
            service_account: 'github-deploy-labordata@labordata.iam.gserviceaccount.com'
      - name: Set up Cloud Run
        uses: google-github-actions/setup-gcloud@v0
      - name: Deploy to Cloudrun
        run: |
          gcloud config set run/region us-central1
          gcloud config set project labordata          
          datasette publish cloudrun f7.db nlrb.db opdr.db old_nlrb.db voluntary_recognitions.db work_stoppages.db lm20.db chips.db \
            --memory 8Gi \
            --cpu 2 \
            --service warehouse \
            -m warehouse_metadata.yml \
            --extra-options="--crossdb --setting sql_time_limit_ms 100000 --cors --setting facet_time_limit_ms 500 --setting allow_facet off --setting trace_debug 1 --setting max_returned_rows 5000" \
            --install=datasette-atom \
            --install=datasette-rure \
            --install=pysqlite3-binary \
            --install=datasette-hashed-urls \
            --install=datasette-block-robots \
            --install=datasette-pretty-traces \
            --version-note=$GITHUB_RUN_NUMBER
